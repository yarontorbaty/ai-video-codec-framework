# üöÄ Step-by-Step Deployment Guide - GPU Worker Launch

**Goal**: Launch GPU worker and run your first autonomous experiment

**Time Required**: 30-45 minutes

**Current Status**: ‚úÖ All code ready, ‚úÖ Tests passed, ‚è≥ Need to launch GPU worker

---

## üìã Prerequisites Checklist

Before starting, verify you have:

- [x] AWS account with admin access
- [x] AWS CLI configured (verified ‚úÖ)
- [x] Anthropic or OpenAI API key
- [x] Test video in S3 (or can upload one)

**Your AWS Account**: 580473065386 ‚úÖ

---

## üéØ Deployment Options

Choose your path:

### Option A: Manual AWS Console (Recommended for First Time)
**Time**: 20 minutes  
**Difficulty**: Easy (point and click)  
**Best for**: Learning and understanding the setup

### Option B: AWS CLI (Faster)
**Time**: 10 minutes  
**Difficulty**: Medium (command line)  
**Best for**: Quick deployment

### Option C: CloudFormation (Automated)
**Time**: 5 minutes  
**Difficulty**: Easy (fully automated)  
**Best for**: Production deployments

---

## üñ•Ô∏è OPTION A: Manual AWS Console (Start Here)

### Step 1: Launch GPU Worker EC2 Instance (10 min)

**1.1 Open AWS EC2 Console**
```
https://console.aws.amazon.com/ec2/
Region: us-east-1
```

**1.2 Click "Launch Instance"**

**1.3 Configure Instance:**

```yaml
Name: ai-codec-gpu-worker-1

AMI: Deep Learning AMI (Ubuntu 22.04)
  - Search for: "Deep Learning AMI GPU PyTorch"
  - Select: Latest Ubuntu 22.04 version
  - Includes: CUDA, cuDNN, PyTorch pre-installed

Instance Type: g4dn.xlarge
  - 4 vCPUs
  - 16 GB RAM
  - 1x NVIDIA T4 GPU (16 GB)
  - Cost: $0.526/hour on-demand ($0.158/hour spot)

Key Pair:
  - Select existing or create new
  - Download .pem file if new

Network:
  - VPC: Default VPC
  - Subnet: Any (us-east-1a recommended)
  - Auto-assign public IP: Enable

Security Group:
  - Create new: ai-codec-gpu-worker-sg
  - Rule: SSH (22) from Your IP
  - Note: No inbound from internet needed (uses SQS)

IAM Role:
  - Create new role: ai-codec-gpu-worker-role
  - Attach policies:
    ‚Ä¢ AmazonSQSFullAccess
    ‚Ä¢ AmazonDynamoDBFullAccess
    ‚Ä¢ AmazonS3FullAccess
  - Or use existing role if you have one

Storage:
  - 100 GB gp3 SSD
  - (Default 125 GB is fine too)

Advanced:
  - Spot instance: Optional (70% savings)
  - Termination protection: Disable (for testing)
```

**1.4 Click "Launch Instance"**

**1.5 Wait 2-3 minutes for instance to start**

**1.6 Copy Public IP address** (you'll need this)

---

### Step 2: Connect to GPU Worker (2 min)

**2.1 Open Terminal**

**2.2 Set Key Permissions**
```bash
chmod 400 ~/Downloads/your-key.pem
```

**2.3 SSH into Instance**
```bash
ssh -i ~/Downloads/your-key.pem ubuntu@<GPU-WORKER-PUBLIC-IP>

# Example:
# ssh -i ~/Downloads/ai-codec-key.pem ubuntu@54.123.45.67
```

**2.4 Verify GPU**
```bash
nvidia-smi

# Should show:
# +-----------------------------------------------------------------------------+
# | NVIDIA-SMI 525.xx.xx    Driver Version: 525.xx.xx    CUDA Version: 12.0   |
# |-------------------------------+----------------------+----------------------+
# | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
# | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
# |===============================+======================+======================|
# |   0  Tesla T4            Off  | 00000000:00:1E.0 Off |                    0 |
# | N/A   35C    P0    26W /  70W |      0MiB / 15360MiB |      0%      Default |
# +-------------------------------+----------------------+----------------------+
```

‚úÖ **Checkpoint**: If you see NVIDIA T4 GPU info, you're good!

---

### Step 3: Setup GPU Worker (5 min)

**3.1 Clone Repository**
```bash
cd ~
git clone https://github.com/yarontorbaty/ai-video-codec-framework.git
cd ai-video-codec-framework
```

**3.2 Create Virtual Environment**
```bash
python3 -m venv venv
source venv/bin/activate
```

**3.3 Install Dependencies**
```bash
# Core dependencies
pip3 install --upgrade pip
pip3 install boto3 numpy opencv-python

# PyTorch with CUDA support (IMPORTANT!)
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Quality metrics
pip3 install scikit-image thop

# Verify PyTorch CUDA
python3 -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
# Should print: CUDA available: True
```

**3.4 Configure AWS Credentials**
```bash
aws configure

# Enter:
# AWS Access Key ID: <your-access-key>
# AWS Secret Access Key: <your-secret-key>
# Default region: us-east-1
# Default output format: json
```

**3.5 Test AWS Connectivity**
```bash
# Test SQS
aws sqs get-queue-attributes \
  --queue-url https://sqs.us-east-1.amazonaws.com/580473065386/ai-video-codec-training-queue \
  --attribute-names ApproximateNumberOfMessages

# Should return queue attributes (0 messages)
```

‚úÖ **Checkpoint**: If AWS commands work, you're ready!

---

### Step 4: Upload Test Video to S3 (2 min)

**4.1 Check if video exists**
```bash
aws s3 ls s3://ai-video-codec-videos-580473065386/test_data/
```

**4.2 If not, upload one**

Option A: Use existing video on orchestrator
```bash
# On orchestrator machine
aws s3 cp test_data/SOURCE_HD_RAW.mp4 \
  s3://ai-video-codec-videos-580473065386/test_data/SOURCE_HD_RAW.mp4
```

Option B: Create test video on GPU worker
```bash
# Generate 10 second test video (1080p30)
python3 << 'EOF'
import cv2
import numpy as np

fps = 30
duration = 10
width, height = 1920, 1080
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
out = cv2.VideoWriter('test_video.mp4', fourcc, fps, (width, height))

for i in range(fps * duration):
    # Create gradient pattern
    frame = np.zeros((height, width, 3), dtype=np.uint8)
    frame[:, :, 0] = (i * 255 // (fps * duration))  # Blue channel changes
    frame[:, :, 1] = 128  # Green constant
    frame[:, :, 2] = 255 - (i * 255 // (fps * duration))  # Red changes
    out.write(frame)

out.release()
print("Generated test_video.mp4")
EOF

# Upload to S3
aws s3 cp test_video.mp4 \
  s3://ai-video-codec-videos-580473065386/test_data/SOURCE_HD_RAW.mp4
```

‚úÖ **Checkpoint**: Video in S3

---

### Step 5: Start GPU Worker (1 min)

**5.1 Start Worker Process**
```bash
cd ~/ai-video-codec-framework
source venv/bin/activate

# Start worker (runs in foreground)
python3 workers/neural_codec_gpu_worker.py

# You should see:
# ================================================================================
# üöÄ NEURAL CODEC GPU WORKER STARTED
#    Worker ID: ip-172-31-xx-xx-12345
#    GPU: NVIDIA T4
#    Device: cuda
#    Queue: https://sqs.us-east-1.amazonaws.com/.../ai-video-codec-training-queue
# ================================================================================
# 
# üì• Polling for neural codec experiments...
#    No jobs available, waiting...
```

**5.2 Keep this terminal open** (worker is now running and polling SQS)

‚úÖ **Checkpoint**: Worker polling for jobs

---

### Step 6: Start Orchestrator (3 min)

**6.1 Open NEW terminal on your LOCAL machine**

**6.2 Set LLM API Key**
```bash
# For Anthropic Claude
export ANTHROPIC_API_KEY=sk-ant-api03-...

# OR for OpenAI GPT
export OPENAI_API_KEY=sk-...
```

**6.3 Navigate to Project**
```bash
cd /Users/yarontorbaty/Documents/Code/AiV1
```

**6.4 Start Orchestrator**
```bash
python3 src/agents/gpu_first_orchestrator.py

# You should see:
# ================================================================================
# üöÄ GPU-FIRST ORCHESTRATOR
#    Iteration: 1
#    No local execution - all work dispatched to GPU workers
# ================================================================================
# üöÄ GPU-FIRST EXPERIMENT CYCLE 1
#    ID: gpu_exp_1234567890
#    Timestamp: 1234567890
# ================================================================================
#
# üìê PHASE 1: DESIGN (Orchestrator)
#    Analyzing past experiments...
```

‚úÖ **Checkpoint**: Orchestrator running

---

### Step 7: Watch First Experiment! (5-10 min)

**What happens now:**

**Terminal 1 (GPU Worker):**
```
üì• Polling for neural codec experiments...

üéØ Received job: gpu_exp_1234567890

üî¨ Executing experiment: gpu_exp_1234567890
   Device: cuda
  üì• Loading video from s3://...
  ‚úÖ Loaded 300 frames at 1080x1920
  üóúÔ∏è  Executing encoding agent...
  üé¨ Encoding video: 1x300 frames at 1080x1920
    üìä Analyzing scene...
    Scene: moderate_motion, Complexity: 0.65, Motion: 0.45
    ‚Üí Strategy: hybrid_semantic (default)
    üéûÔ∏è  Selected 11 I-frames from 300 total frames
    üóúÔ∏è  Compressing I-frames...
    üìù Generating semantic description...
    ‚úÖ Encoding complete
  ‚úÖ Encoding complete
     Bitrate: 0.8432 Mbps
     Strategy: hybrid_semantic
  üîÑ Executing decoding agent...
  üé¨ Decoding video...
    üéûÔ∏è  Decoding 11 I-frames...
    üé® Generating 300 frames...
    ‚ú® Enhancing temporal consistency...
    ‚úÖ Decoding complete
  ‚úÖ Decoding complete
     Decode FPS: 28.3
     TOPS/frame: 1.12
  üìä Calculating quality metrics...
  ‚úÖ Quality metrics:
     PSNR: 36.42 dB
     SSIM: 0.9573
‚úÖ Experiment completed in 142.3s
   Bitrate: 0.8432 Mbps
   PSNR: 36.42 dB
   SSIM: 0.9573
   TOPS: 1.12 per frame
```

**Terminal 2 (Orchestrator):**
```
üì§ PHASE 2: DISPATCH TO GPU
   Sending experiment to GPU worker queue...
   ‚úÖ Dispatched to SQS
   Message ID: abc123...

‚è≥ PHASE 3: WAITING FOR GPU WORKER
   Polling for results (timeout: 1800s)...
   ‚è≥ Still waiting... (30s / 1800s)
   ‚è≥ Still waiting... (60s / 1800s)
   ‚úÖ GPU execution complete
   Bitrate: 0.8432 Mbps
   PSNR: 36.42 dB

üìä PHASE 4: ANALYSIS
   Evaluating results against targets...
   üìà Metrics:
      Bitrate: 0.8432 Mbps (target: ‚â§1.0 Mbps) ‚úÖ
      PSNR: 36.42 dB (target: ‚â•35.0 dB) ‚úÖ
      SSIM: 0.9573 (target: ‚â•0.95) ‚úÖ
      TOPS: 1.12 (target: ‚â§1.33) ‚úÖ
      Compression: 127.3x
      Bitrate reduction: 91.6% vs HEVC
   üìù Blog post updated with results

‚úÖ EXPERIMENT CYCLE COMPLETE
   Total time: 156.2s
   Success: True
```

---

## üéâ SUCCESS! What Just Happened

Your autonomous AI video codec system just:

1. ‚úÖ **Designed** an experiment (LLM generated neural architecture)
2. ‚úÖ **Dispatched** to GPU worker (via SQS)
3. ‚úÖ **Compressed** video using EncodingAgent on GPU
4. ‚úÖ **Reconstructed** video using DecodingAgent
5. ‚úÖ **Measured** quality (PSNR, SSIM, TOPS)
6. ‚úÖ **Achieved** all targets on first try! üéâ

---

## üìä Expected First Results

| Metric | Target | Likely Result | Status |
|--------|--------|---------------|--------|
| Bitrate | ‚â§1.0 Mbps | 0.8-1.2 Mbps | üéØ Close! |
| PSNR | ‚â•35 dB | 34-38 dB | üéØ Likely! |
| SSIM | ‚â•0.95 | 0.93-0.96 | üéØ Close! |
| TOPS | ‚â§1.33 | 1.0-1.3 | ‚úÖ Yes! |

**If not all targets met on first try**: That's normal! The system will iterate and improve automatically.

---

## üîÑ Run More Experiments

**Let it evolve!** Run the orchestrator again:

```bash
python3 src/agents/gpu_first_orchestrator.py
```

Each iteration learns from previous results and tries improvements.

**Expected evolution:**
- Iteration 1: Baseline (may miss some targets)
- Iteration 5: Improving
- Iteration 10: **Targets achieved!** ‚úÖ
- Iteration 25: Excellent performance

---

## üìà Monitor Progress

### View Results in DynamoDB

```bash
aws dynamodb scan \
  --table-name ai-video-codec-experiments \
  --filter-expression "begins_with(experiment_id, :prefix)" \
  --expression-attribute-values '{":prefix":{"S":"gpu_exp_"}}' \
  --max-items 5
```

### View Dashboard

Open `dashboard/index.html` in your browser to see:
- Real-time experiment status
- Bitrate trends over time
- Quality metrics (PSNR/SSIM)
- Success/failure rates

---

## üõë When Done Testing

### Stop GPU Worker
```bash
# In GPU worker terminal
Ctrl+C

# Then exit
exit
```

### Stop EC2 Instance (Save Money!)
```bash
# From your local machine
aws ec2 stop-instances --instance-ids i-xxxxx

# Or use AWS Console
# EC2 ‚Üí Instances ‚Üí Select instance ‚Üí Instance State ‚Üí Stop
```

**Note**: Stopped instances don't incur compute charges, only EBS storage ($0.10/GB/month)

---

## üí∞ Cost Summary

**First experiment**:
- GPU time: ~5 minutes
- Cost: ~$0.04

**Full testing (50 experiments)**:
- GPU time: ~4 hours
- Cost: ~$2

**Monthly (4 hours/day)**:
- On-demand: $63/month
- Spot: $19/month (70% savings!)

---

## üêõ Troubleshooting

### Issue: "CUDA not available"
```bash
# Reinstall PyTorch with CUDA
pip3 uninstall torch torchvision torchaudio
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### Issue: "Cannot access SQS queue"
```bash
# Check IAM role attached to EC2 instance
# Go to EC2 Console ‚Üí Select instance ‚Üí Actions ‚Üí Security ‚Üí Modify IAM role
# Attach role with SQS, DynamoDB, S3 permissions
```

### Issue: "Orchestrator timeout"
```bash
# Check GPU worker is running
ssh ubuntu@<GPU-IP>
ps aux | grep neural_codec_gpu_worker
```

### Issue: "Out of GPU memory"
```bash
# Reduce batch size or use smaller resolution in config
# Or upgrade to g4dn.2xlarge (32GB GPU)
```

---

## ‚úÖ Quick Reference

### GPU Worker Commands
```bash
# Start worker
cd ~/ai-video-codec-framework
source venv/bin/activate
python3 workers/neural_codec_gpu_worker.py

# Check status
nvidia-smi
ps aux | grep neural_codec_gpu_worker
```

### Orchestrator Commands
```bash
# Start orchestrator (one experiment)
export ANTHROPIC_API_KEY=sk-ant-...
python3 src/agents/gpu_first_orchestrator.py

# Run multiple iterations
for i in {1..10}; do
  echo "Iteration $i"
  python3 src/agents/gpu_first_orchestrator.py
  sleep 10
done
```

### AWS Commands
```bash
# Check queue
aws sqs get-queue-attributes --queue-url <URL> --attribute-names All

# Check experiments
aws dynamodb scan --table-name ai-video-codec-experiments --max-items 5

# Check S3
aws s3 ls s3://ai-video-codec-videos-580473065386/test_data/
```

---

## üéØ Next Steps After First Success

1. **Run 10 experiments** - Let it learn and improve
2. **Analyze results** - Which strategies work best?
3. **Optimize costs** - Switch to spot instances
4. **Scale up** - Add more GPU workers for parallel experiments
5. **Deploy to edge** - Export decoder to mobile devices

---

## üìû Need Help?

**Documentation**:
- [Quick Start Guide](GPU_NEURAL_CODEC_QUICKSTART.md)
- [Architecture Guide](GPU_NEURAL_CODEC_ARCHITECTURE.md)
- [Troubleshooting](GPU_NEURAL_CODEC_QUICKSTART.md#common-issues--solutions)

**Verification**:
```bash
./scripts/verify_v2.sh
python3 scripts/test_v2_components.py
```

---

## üéâ You're Ready!

Follow the steps above and you'll have your autonomous AI video codec running in 30-45 minutes!

**Good luck!** üöÄ

---

**Questions?** Each step is designed to be copy-paste friendly. If you hit an issue, check the troubleshooting section or run the verification scripts.

